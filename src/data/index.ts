import type { Project, SkillCategory, Experience, Publication, Stat } from '../types';

export const personalInfo = {
  name: 'Rayan Diatsa',
  title: 'ML & Data Science Student ',
  tagline: 'Building intelligent systems at the intersection of research and production.',
  bio: `Passionate ML Engineer with 2+ years turning complex data into intelligent systems. I specialize in deep learning architectures, NLP, and scalable MLOps pipelines. My work spans from crafting novel transformer-based models to deploying production-grade AI at scale.`,
  location: 'Yaounde, Cameroun',
  email: 'rayandiatsa@gmail.com',
  available: true,
  github: 'https://github.com/RayanCoder681',
  linkedin: 'https://linkedin.com/in/rayan-diatsa-0a539734a',
  // twitter: 'https://twitter.com',
  scholar: 'https://scholar.google.com',
  // huggingface: 'https://huggingface.co',
};

export const stats: Stat[] = [
  { label: 'Models deployed', value: '10', suffix: '+', description: 'Production ML models' },
  { label: 'Research papers', value: '10', suffix: '', description: 'Peer-reviewed publications' },
  { label: 'Years experience', value: '2', suffix: '+', description: 'In ML/DS' },
  { label: 'Open source stars', value: '3.2k', suffix: '', description: 'GitHub stars' },
];

export const projects: Project[] = [
  {
    id: 'neural-llm-fine-tuning',
    title: 'LLaMA Fine-Tuning Framework',
    subtitle: 'Production-grade LLM fine-tuning with LoRA & QLoRA',
    description: 'End-to-end framework for fine-tuning large language models using PEFT techniques. Achieves 4x memory reduction with QLoRA while maintaining 98% of base model performance.',
    longDescription: `A comprehensive framework designed to simplify the fine-tuning of open-source LLMs for domain-specific tasks. 
    The system integrates LoRA and QLoRA adapters, automatic mixed precision training, gradient checkpointing, 
    and a streamlined evaluation pipeline. Deployed at scale serving 10k+ daily inference requests.`,
    tags: ['LLM', 'LoRA', 'Fine-tuning', 'NLP', 'Transformers'],
    category: 'nlp',
    metrics: [
      { label: 'Memory reduction', value: '4Ã—' },
      { label: 'Perplexity improvement', value: '34%' },
      { label: 'Training speed', value: '2.8Ã—' },
    ],
    technologies: ['Python', 'PyTorch', 'HuggingFace', 'PEFT', 'bitsandbytes', 'WandB', 'FastAPI'],
    github: 'https://github.com',
    demo: 'https://demo.example.com',
    image: 'llm',
    featured: true,
    year: 2024,
  },
  {
    id: 'vision-transformer-med',
    title: 'MedViT â€” Medical Image Segmentation',
    subtitle: 'Vision Transformer for tumor detection & segmentation',
    description: 'Custom ViT architecture for medical imaging achieving state-of-the-art results on BraTS 2023 benchmark. Dice score of 0.924 on brain tumor segmentation.',
    longDescription: `MedViT is a specialized Vision Transformer for 3D medical image analysis. 
    The model incorporates multi-scale feature extraction, attention-based skip connections, 
    and a novel data augmentation strategy for limited medical datasets. 
    Validated on BraTS 2023, LiTS, and KiTS23 benchmarks.`,
    tags: ['Computer Vision', 'ViT', 'Medical AI', 'Segmentation', 'Deep Learning'],
    category: 'computer-vision',
    metrics: [
      { label: 'Dice Score', value: '0.924' },
      { label: 'Sensitivity', value: '96.2%' },
      { label: 'Inference time', value: '120ms' },
    ],
    technologies: ['Python', 'PyTorch', 'MONAI', 'SimpleITK', 'NumPy', 'TensorBoard'],
    github: 'https://github.com',
    paper: 'https://arxiv.org',
    image: 'medvit',
    featured: true,
    year: 2024,
  },
  {
    id: 'mlops-platform',
    title: 'NeuralFlow MLOps Platform',
    subtitle: 'End-to-end ML lifecycle management system',
    description: 'Internal MLOps platform handling model versioning, A/B testing, drift detection, and automated retraining pipelines for 50+ production models.',
    longDescription: `NeuralFlow is a production MLOps platform built to handle the full lifecycle of ML models. 
    Features include feature store integration, model registry, real-time monitoring with drift detection, 
    shadow deployment, and automated retraining triggers via Airflow DAGs.`,
    tags: ['MLOps', 'Kubernetes', 'Feature Store', 'Monitoring', 'CI/CD'],
    category: 'mlops',
    metrics: [
      { label: 'Models managed', value: '50+' },
      { label: 'Deployment time', value: 'âˆ’85%' },
      { label: 'MTTR', value: '<5min' },
    ],
    technologies: ['Python', 'FastAPI', 'Airflow', 'MLflow', 'Kubernetes', 'Prometheus', 'Grafana', 'Redis', 'PostgreSQL'],
    github: 'https://github.com',
    image: 'mlops',
    featured: true,
    year: 2023,
  },
  {
    id: 'graph-neural-recommender',
    title: 'Graph Neural Recommender System',
    subtitle: 'GNN-based collaborative filtering at scale',
    description: 'Production recommender system using Graph Neural Networks for a 10M+ user platform, achieving 28% improvement in CTR over matrix factorization baseline.',
    longDescription: `A graph-based recommendation engine that models user-item interactions as a heterogeneous graph. 
    Using GraphSAGE with attention mechanisms, the system captures high-order connectivity patterns. 
    Deployed with Faiss for approximate nearest-neighbor search, serving real-time recommendations at <50ms latency.`,
    tags: ['GNN', 'Recommender Systems', 'Graph ML', 'PyTorch Geometric'],
    category: 'deep-learning',
    metrics: [
      { label: 'CTR improvement', value: '+28%' },
      { label: 'Latency p99', value: '48ms' },
      { label: 'Users', value: '10M+' },
    ],
    technologies: ['Python', 'PyG', 'Faiss', 'Redis', 'Spark', 'Kafka'],
    github: 'https://github.com',
    image: 'gnn',
    featured: false,
    year: 2023,
  },
  {
    id: 'rl-trading',
    title: 'RL Portfolio Optimization',
    subtitle: 'Deep Reinforcement Learning for algorithmic trading',
    description: 'PPO + Transformer-based RL agent for multi-asset portfolio optimization. Outperforms S&P 500 benchmark by 18.4% annualized return in backtesting.',
    longDescription: `A deep RL agent using Proximal Policy Optimization with a Transformer-based policy network 
    for portfolio management. The state space encodes technical indicators, order book data, and macro features. 
    Trained on 20 years of historical data across 500+ assets.`,
    tags: ['Reinforcement Learning', 'FinTech', 'PPO', 'Time Series', 'Transformers'],
    category: 'reinforcement-learning',
    metrics: [
      { label: 'Annual return', value: '+18.4%' },
      { label: 'Sharpe ratio', value: '1.84' },
      { label: 'Max drawdown', value: 'âˆ’12%' },
    ],
    technologies: ['Python', 'PyTorch', 'Stable-Baselines3', 'Ray RLlib', 'Pandas', 'QuantLib'],
    github: 'https://github.com',
    image: 'rl',
    featured: false,
    year: 2022,
  },
  {
    id: 'multimodal-rag',
    title: 'Multimodal RAG Pipeline',
    subtitle: 'Retrieval-Augmented Generation with vision + text',
    description: 'RAG system combining CLIP embeddings and LLM for cross-modal retrieval over 2M+ document/image pairs. Powers an enterprise knowledge base serving 5k+ daily queries.',
    longDescription: `A production multimodal RAG pipeline that handles both text and image queries. 
    Uses CLIP for joint embedding, ChromaDB for vector storage, and a fine-tuned LLaMA model for generation. 
    Implements hybrid dense/sparse retrieval with re-ranking via cross-encoders.`,
    tags: ['RAG', 'Multimodal', 'CLIP', 'LLM', 'Vector DB', 'NLP'],
    category: 'nlp',
    metrics: [
      { label: 'Retrieval accuracy', value: '91.3%' },
      { label: 'Response time', value: '1.2s' },
      { label: 'Documents indexed', value: '2M+' },
    ],
    technologies: ['Python', 'LangChain', 'CLIP', 'ChromaDB', 'FastAPI', 'Docker'],
    github: 'https://github.com',
    demo: 'https://demo.example.com',
    image: 'rag',
    featured: false,
    year: 2024,
  },
];

export const skillCategories: SkillCategory[] = [
  {
    title: 'Deep Learning',
    icon: 'ðŸ§ ',
    skills: [
      { name: 'PyTorch', level: 10, category: 'framework' },
      { name: 'TensorFlow / Keras', level: 10, category: 'framework' },
      { name: 'JAX / Flax', level: 2, category: 'framework' },
      { name: 'Transformers (HuggingFace)', level: 2, category: 'framework' },
      { name: 'CNNs / ViT / DETR', level: 2, category: 'concept' },
      { name: 'LLMs / Fine-tuning', level: 2, category: 'concept' },
    ],
  },
  {
    title: 'Data Science & ML',
    icon: 'ðŸ“Š',
    skills: [
      { name: 'Scikit-learn', level: 50, category: 'framework' },
      { name: 'XGBoost / LightGBM', level: 2, category: 'framework' },
      { name: 'Pandas / Polars', level: 80, category: 'tool' },
      { name: 'Statistical Modeling', level: 80, category: 'concept' },
      { name: 'Feature Engineering', level: 10, category: 'concept' },
      { name: 'Time Series Analysis', level: 2, category: 'concept' },
    ],
  },
  {
    title: 'MLOps & Infrastructure',
    icon: 'âš™ï¸',
    skills: [
      { name: 'MLflow / DVC', level: 2, category: 'tool' },
      { name: 'Kubernetes / Docker', level: 2, category: 'tool' },
      { name: 'Apache Airflow', level: 2, category: 'tool' },
      { name: 'Prometheus / Grafana', level: 2, category: 'tool' },
      { name: 'CI/CD (GitHub Actions)', level: 2, category: 'tool' },
      { name: 'Model Serving (Triton)', level: 2, category: 'tool' },
    ],
  },
  {
    title: 'Programming',
    icon: 'ðŸ’»',
    skills: [
      { name: 'Python', level: 70, category: 'language' },
      { name: 'SQL / NoSQL', level: 15, category: 'language' },
      { name: 'Html / Css ', level: 98, category: 'language' },
      { name: 'javaScript', level: 40, category: 'language' },
      { name: 'TypeScript', level: 30, category: 'language' },
      { name: 'Bash / Linux', level: 10, category: 'language' },
    ],
  },
  {
    title: 'Cloud & Data Platforms',
    icon: 'â˜ï¸',
    skills: [
      { name: 'AWS (SageMaker, S3, EC2)', level: 2, category: 'cloud' },
      { name: 'GCP (Vertex AI, BigQuery)', level: 2, category: 'cloud' },
      { name: 'Azure ML', level: 2, category: 'cloud' },
      { name: 'Apache Spark / Databricks', level: 2, category: 'tool' },
      { name: 'Kafka / Flink', level: 2, category: 'tool' },
      { name: 'dbt / Airflow', level: 2, category: 'tool' },
    ],
  },
  {
    title: 'NLP & Vision Specializations',
    icon: 'ðŸ‘ï¸',
    skills: [
      { name: 'RAG / Vector DBs', level: 10, category: 'concept' },
      { name: 'RLHF / Alignment', level: 2, category: 'concept' },
      { name: 'Object Detection / YOLO', level: 2, category: 'concept' },
      { name: 'Image Segmentation', level: 40, category: 'concept' },
      { name: 'Graph Neural Networks', level: 40, category: 'concept' },
      { name: 'Diffusion Models', level: 10, category: 'concept' },
    ],
  },
];

export const experiences: Experience[] = [
  {
    id: 'exp1',
    company: 'Personal-Work',
    role: 'Machine and Deep Learning Student',
    period: 'Feb 2026 â€” Present',
    location: 'Yaounde, Cameroun',
    type: 'full-time',
    description: 'Learning and then write from scratch an artificial neuron',
    achievements: [
      'Implement and demonstrate the differents formulas for the neuron',
      'Implement and demonstrate the differents formulas for the perceptron',
      'Implement and demonstrate the differents formulas for the multi-layer perceptron',
    ],
    stack: ['Python', 'Algebra Skills', 'Numpy', 'Pandas', 'Matplotlib', 'sklearn'],
  },
  {
    id: 'exp2',
    company: 'Personal-Work',
    role: 'Machine Learning Student',
    period: 'Jan 2026 Feb-2026',
    location: 'Yaounde, Cameroun',
    type: 'full-time',
    description: 'Learning and then write from the scratch the main ML algorithms',
    achievements: [
      'Implement and Write the code of the Linear regression using the mean-squared method',
      'Implement and Write the code of the Logistic regression using the gradient descent method',
      'Implement and Write the code of the Neural network using the backpropagation method',
    ],
    stack: ['Python', 'Numpy', 'Pandas', 'Matplotlib'],
  },
  // {
  //   id: 'exp3',
  //   company: 'Criteo AI Lab',
  //   role: 'Data Scientist â€” RecSys',
  //   period: 'Jun 2020 â€” Feb 2022',
  //   location: 'Paris, France',
  //   type: 'full-time',
  //   description: 'Built and maintained large-scale recommendation systems serving billions of daily ad impressions.',
  //   achievements: [
  //     'Redesigned CTR prediction model with GNN achieving +15% CTR lift across major campaigns',
  //     'Deployed real-time feature pipeline on Kafka + Flink processing 500k events/second',
  //     'Led A/B testing framework reducing experiment cycle time from 2 weeks to 3 days',
  //     'Mentored 3 junior data scientists; established ML best practices documentation',
  //   ],
  //   stack: ['Python', 'PyTorch', 'Spark', 'Kafka', 'Flink', 'Hive', 'Airflow'],
  // },
  // {
  //   id: 'exp4',
  //   company: 'INRIA',
  //   role: 'Research Intern â€” Machine Learning',
  //   period: 'Feb 2020 â€” May 2020',
  //   location: 'Sophia Antipolis, France',
  //   type: 'internship',
  //   description: 'Research on federated learning and differential privacy for healthcare data.',
  //   achievements: [
  //     'Implemented FL algorithm with DP guarantees (Îµ=1.0) across 10 hospital nodes',
  //     'Achieved 94% of centralized baseline accuracy while preserving patient privacy',
  //     'Co-authored workshop paper at ICLR 2020 on privacy-preserving medical AI',
  //   ],
  //   stack: ['Python', 'TensorFlow-Federated', 'PySyft', 'NumPy'],
  // },
];

export const publications: Publication[] = [
  {
    id: 'pub1',
    title: 'Efficient Long-Context Transformers via Hierarchical Sparse Attention',
    authors: ['Alex Chen', 'Maria Santos', 'Yuki Tanaka', 'David Kim'],
    venue: 'NeurIPS 2023',
    year: 2023,
    type: 'conference',
    abstract: 'We propose HierSparse, a hierarchical sparse attention mechanism that reduces the computational complexity of self-attention from O(nÂ²) to O(n log n) while maintaining full expressiveness for long-context tasks. Evaluated across 7 benchmarks, HierSparse matches full attention performance with 4Ã— speed improvement at 16k context length.',
    arxiv: 'https://arxiv.org/abs/2312.xxxxx',
    citations: 127,
    tags: ['Transformers', 'Attention', 'Efficiency', 'LLM'],
  },
  {
    id: 'pub2',
    title: 'MedViT-3D: Vision Transformers for Volumetric Medical Image Segmentation',
    authors: ['Alex Chen', 'Sophie Dubois', 'Ahmed Hassan'],
    venue: 'Medical Image Analysis',
    year: 2023,
    type: 'journal',
    abstract: 'We introduce MedViT-3D, a pure Vision Transformer architecture for 3D medical image segmentation. Our approach uses 3D window attention with shifted partitioning, achieving state-of-the-art results on BraTS 2023, LiTS, and KiTS23 benchmarks without CNN priors.',
    doi: '10.1016/j.media.2023.xxxxx',
    arxiv: 'https://arxiv.org/abs/2308.xxxxx',
    citations: 89,
    tags: ['Medical AI', 'ViT', 'Segmentation', 'Computer Vision'],
  },
  {
    id: 'pub3',
    title: 'Privacy-Preserving Federated Learning for Clinical NLP with Differential Privacy',
    authors: ['Alex Chen', 'Isabelle Moreau', 'Thomas Weber'],
    venue: 'ICLR 2021 Workshop on Healthcare AI',
    year: 2021,
    type: 'workshop',
    abstract: 'We present a federated learning framework for clinical text classification that provides (Îµ, Î´)-differential privacy guarantees. Across 10 hospital nodes, our approach achieves 94% of centralized accuracy with Îµ=1.0, enabling privacy-preserving medical AI at scale.',
    arxiv: 'https://arxiv.org/abs/2104.xxxxx',
    citations: 43,
    tags: ['Federated Learning', 'Differential Privacy', 'NLP', 'Healthcare'],
  },
  {
    id: 'pub4',
    title: 'GraphRec+: Heterogeneous Graph Neural Networks for Session-Based Recommendation',
    authors: ['Alex Chen', 'Wei Liu', 'Carlos Mendez'],
    venue: 'RecSys 2022',
    year: 2022,
    type: 'conference',
    abstract: 'GraphRec+ extends session-based recommendation to heterogeneous graphs, modeling user-item-category-attribute interactions. Our model achieves state-of-the-art performance on five public datasets with a novel temporal graph sampling strategy.',
    arxiv: 'https://arxiv.org/abs/2209.xxxxx',
    citations: 61,
    tags: ['GNN', 'RecSys', 'Graph ML', 'Deep Learning'],
  },
];
